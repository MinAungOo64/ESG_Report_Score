{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9408f8ee-def0-4c4b-8b7a-3cf7d9861c21",
   "metadata": {},
   "source": [
    "# Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ef814-d257-4837-857d-d1a5667997aa",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90e43bc-bb34-48b7-8dfb-bca7ea718c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40428aa4-6c95-4446-b8bb-90919de9d849",
   "metadata": {},
   "source": [
    "### Load the tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "515853dd-9932-4697-b70d-4815d5b48bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset= load_from_disk(\"esg_tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da2105-f7a8-4c8e-b8d2-1f5a1ef62657",
   "metadata": {},
   "source": [
    "### Convert to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ad8a77-8fb9-42a2-b6e8-806d3df34a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bda84-e6de-4994-8628-5cc96b48f95e",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "Batch size can be increased if you have enough vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "633e0cfe-1e11-44c5-834d-e911082a4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# For dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Prepare the data in batch size of 2 with dynamic padding, shuffles at each epoch\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=2, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Prepare validation data in batch size of 2\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"], batch_size=2, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Prepare validation data in batch size of 2\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size=2, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498030a-3bda-494a-8165-43f8cd49db48",
   "metadata": {},
   "source": [
    "## Loading and modifying the model for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ae5c0b-9d47-4c65-8784-b9d7065eba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LongformerForRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pretrained Longformer model (handles up to 4096 tokens)\n",
    "        self.longformer = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "        \n",
    "        # Dropout for regularization (prevents overfitting)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Final regression layer to predict 4 scores: e_score, s_score, g_score, total_score\n",
    "        self.regressor = nn.Linear(self.longformer.config.hidden_size, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through Longformer\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Try to use pooled output (if available)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # If pooled output is None (some models don't provide it), use mean pooling over all tokens\n",
    "        if pooled_output is None:\n",
    "            pooled_output = outputs.last_hidden_state.mean(dim=1)  # shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Pass through regression head to get 4 continuous outputs\n",
    "        return self.regressor(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25552ba-2a4d-4fdb-89ef-5aee834ef279",
   "metadata": {},
   "source": [
    "### Test to see if model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5517051d-0d97-4cd1-92b4-381a89312585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "tensor([[ 0.2830, -0.0491,  0.1723,  0.2213],\n",
      "        [ 0.2815, -0.0459,  0.1947,  0.2289]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = LongformerForRegression()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "batch = next(iter(train_dataloader))  # get one batch\n",
    "batch = {k: v.to(device) for k, v in batch.items()}  # move to device\n",
    "\n",
    "inputs = {k: batch[k] for k in [\"input_ids\", \"attention_mask\"]}  # only model inputs\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.shape)  # should be [batch_size, 4]\n",
    "print(outputs)        # raw predictions for the 4 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961f5a7-8d8f-454e-bb35-f8f9110024a0",
   "metadata": {},
   "source": [
    "As expected 2 batches of scores were outputted, each batch contains 4 scores corresponding to e_score, s_score, g_score, combined_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5a4e-1a33-4d2e-85b2-5b9015ab67ab",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9277c6-f925-49bf-b83e-5f9ea1070e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07123ed5-af0a-4d47-896f-b821a13c232d",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13140b7-e9cf-4ad5-b2f7-cd078df2a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16083\n"
     ]
    }
   ],
   "source": [
    "# Define scheduler to change learning rate\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "# num epochs * num of batches\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", # Type of scheduler (linear decay)\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, # No warm-up period, meaning the learning rate starts at the maximum value right away and decreases linearly\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea88b0-73ca-4385-8555-46dc5d5a005b",
   "metadata": {},
   "source": [
    "## Move Model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f184d-6abf-4241-b4c2-df45b8dce8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if avail or CPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25caec65-5384-400f-8e90-30b34db74c5a",
   "metadata": {},
   "source": [
    "## Set Model to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab7ddf18-401a-41ba-bb21-ca5ca6bd1919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForRegression(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LongformerPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (regressor): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "model.train()  # This ensures layers like dropout are active during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68ac91-8807-4b0c-a948-ee69fb5fd15e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a49d853-a1a0-47c4-8bf3-bd9cc7d7b3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca8581b8a6d41b784d927d1e70f4466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/5361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Training Loss: 30.6857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e88aa2796a54acfa173349b7edbb38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1:   0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 34.4633\n",
      "Validation loss improved — saving model.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23abda63d54245a395e90328c9165823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/5361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Training Loss: 29.0902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75ca3b203a2410dbc9e7b4be6a1cfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2:   0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 34.4729\n",
      "No improvement. Patience: 1/3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fae087556342e29cca4cf1deb0f9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/5361 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Training Loss: 29.0468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3239110ba4894dad953a501465d7d358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3:   0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 34.6825\n",
      "No improvement. Patience: 2/3\n",
      "\n",
      "Loaded best model from saved state.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3         # how many epochs to wait before stopping\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"],\n",
    "            \"attention_mask\": batch[\"attention_mask\"]\n",
    "        }\n",
    "        labels = torch.stack([\n",
    "            batch['e_score'],\n",
    "            batch['s_score'],\n",
    "            batch['g_score'],\n",
    "            batch['total_score']\n",
    "        ], dim=1).float()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"],\n",
    "                \"attention_mask\": batch[\"attention_mask\"]\n",
    "            }\n",
    "            labels = torch.stack([\n",
    "                batch['e_score'],\n",
    "                batch['s_score'],\n",
    "                batch['g_score'],\n",
    "                batch['total_score']\n",
    "            ], dim=1).float()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # === EARLY STOPPING CHECK ===\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(\"Validation loss improved — saving model.\\n\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\\n\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# === Load best model ===\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Loaded best model from saved state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e8baf-39d9-4450-ab02-a0716510b668",
   "metadata": {},
   "source": [
    "# Save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f89bf0-9b8d-4feb-b906-58f5bf8d5371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\\\\tokenizer_config.json',\n",
       " 'C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\\\\special_tokens_map.json',\n",
       " 'C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\\\\vocab.json',\n",
       " 'C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\\\\merges.txt',\n",
       " 'C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\\\\added_tokens.json',\n",
       " 'C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Save model weights\n",
    "output_dir = \"C:/Users/steve/HuggingFace Models/LongFormer_ESG_Score\"\n",
    "torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "# Save tokenizer as usual (since it's from Hugging Face)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
